{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import get_ipython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POSSIBLE_LABELS = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "id2name = {i: name for i, name in enumerate(POSSIBLE_LABELS)}\n",
    "name2id = {name: i for i, name in id2name.items()}\n",
    "len(id2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    pattern = re.compile(\"(.+\\/)?(\\w+)\\/([^_]+)_.+wav\")\n",
    "    all_files = glob(os.path.join(data_dir, 'train/audio/*/*wav'))\n",
    "    \n",
    "    with open(os.path.join(data_dir, 'validation_list.txt'), 'r') as fin:\n",
    "        validation_files = fin.readlines()\n",
    "    valset = set()\n",
    "    for entry in validation_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            valset.add(r.group(3))\n",
    "            \n",
    "    possible = set(POSSIBLE_LABELS)\n",
    "    train, val = [], []\n",
    "    for entry in all_files:\n",
    "        r = re.match(pattern, entry)\n",
    "        if r:\n",
    "            label, uid = r.group(2), r.group(3)\n",
    "            if label == '_silence_':\n",
    "                continue\n",
    "            if label == '_background_noise_':\n",
    "                label = 'silence'\n",
    "            if label not in possible:\n",
    "                label = 'unknown'\n",
    "            \n",
    "            label_id = name2id[label]\n",
    "            \n",
    "            sample = (label, label_id, uid, entry)\n",
    "            if uid in valset:\n",
    "                val.append(sample)\n",
    "            else:\n",
    "                train.append(sample)\n",
    "    columns_list = ['label', 'label_id', 'user_id', 'wav_file']\n",
    "    train_df = pd.DataFrame(train, columns = columns_list)\n",
    "    valid_df = pd.DataFrame(val, columns = columns_list)\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = load_data('./data/')\n",
    "train_df.head()\n",
    "train_df.label.value_counts()\n",
    "silence_files = train_df[train_df.label == 'silence']\n",
    "train_df = train_df[train_df.label != 'silence']\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def read_wav_file(fname):\n",
    "    _, wav = wavfile.read(fname)\n",
    "    wav = wav.astype(np.float32) / np.iinfo(np.int16).max\n",
    "    return wav\n",
    "\n",
    "silence_data = np.concatenate([read_wav_file(x) for x in silence_files.wav_file.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "\n",
    "def process_wav_file(fname):\n",
    "    wav = read_wav_file(fname)\n",
    "    \n",
    "    L = 16000\n",
    "    \n",
    "    if len(wav) > L:\n",
    "        i = np.random.randint(0, len(wav) - len)\n",
    "        wav = wav[i: (i+L)]\n",
    "    elif len(wav) < L:\n",
    "        rem_len = L - len(wav)\n",
    "        i = np.random.randint(0, len(silence_data) - rem_len)\n",
    "        silence_part = silence_data[i: (i+L)]\n",
    "        j = np.random.randint(0, rem_len)\n",
    "        silence_part_left = silence_part[0: j]\n",
    "        silence_part_right = silence_part[j: rem_len]\n",
    "        wav = np.concatenate([silence_part_left, wav, silence_part_right])\n",
    "    \n",
    "    specgram = stft(wav, 16000, nperseg = 400, noverlap = 240, nfft = 512, padded = False, boundary = None)\n",
    "    phase = np.angle(specgram[2]) / np.pi\n",
    "    amp = np.log1p(np.abs(specgram[2]))\n",
    "    \n",
    "    return np.stack([phase, amp], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(train_batch_size):\n",
    "    while True:\n",
    "        this_train = train_df.groupby('label_id').apply(lambda x: x.sample(n = 100))\n",
    "        shuffled_ids = random.sample(range(this_train.shape[0]), this_train.shape[0])\n",
    "        for start in range(0, len(shuffled_ids), train_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + train_batch_size, len(shuffled_ids))\n",
    "            i_train_batch = shuffled_ids[start: end]\n",
    "            for i in i_train_batch:\n",
    "                x_batch.append(process_wav_file(this_train.wav_file.values[i]))\n",
    "                y_batch.append(this_train.label_id.values[i])\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = np_utils.to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch\n",
    "            \n",
    "def valid_generator(val_batch_size):\n",
    "    while True:\n",
    "        ids = list(range(valid_df.shape[0]))\n",
    "        for start in range(0, len(ids), val_batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + val_batch_size, len(ids))\n",
    "            i_val_batch = ids[start: end]\n",
    "            for i in i_val_batch:\n",
    "                x_batch.append(process_wav_file(valid_df.wav_file.values[i]))\n",
    "                y_batch.append(valid_df.label_id.values[i])\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = np_utils.to_categorical(y_batch, num_classes = len(POSSIBLE_LABELS))\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 257, 98, 2)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 257, 98, 2)   8           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 255, 96, 16)  304         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 255, 96, 16)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 255, 96, 16)  64          activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 127, 48, 16)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 125, 46, 32)  4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 125, 46, 32)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 125, 46, 32)  128         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 62, 23, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 60, 21, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 60, 21, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 60, 21, 64)   256         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 10, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 8, 128)   73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 28, 8, 128)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 8, 128)   512         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 14, 4, 128)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 4, 64)    8256        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 64)           0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling2d_1[0][0] \n",
      "                                                                 global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           8256        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12)           780         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 115,556\n",
      "Trainable params: 115,072\n",
      "Non-trainable params: 484\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_in = Input(shape = (257, 98, 2))\n",
    "x = BatchNormalization()(x_in)\n",
    "for i in range(4):\n",
    "    x = Conv2D(16*(2 ** i), (3,3))(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "x = Conv2D(64, (1,1))(x)\n",
    "x_branch_1 = GlobalAveragePooling2D()(x)\n",
    "x_branch_2 = GlobalMaxPool2D()(x)\n",
    "x = concatenate([x_branch_1, x_branch_2])\n",
    "x = Dense(64, activation = 'relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(len(POSSIBLE_LABELS), activation = 'sigmoid')(x)\n",
    "model = Model(inputs = x_in, outputs = x)\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "344/344 [==============================] - 44s 128ms/step - loss: 2.3021 - acc: 0.1593\n",
      "Epoch 2/50\n",
      "344/344 [==============================] - 43s 125ms/step - loss: 1.6733 - acc: 0.4009\n",
      "Epoch 3/50\n",
      "344/344 [==============================] - 44s 128ms/step - loss: 1.0735 - acc: 0.6344\n",
      "Epoch 4/50\n",
      "344/344 [==============================] - 43s 126ms/step - loss: 0.7943 - acc: 0.7274\n",
      "Epoch 5/50\n",
      "344/344 [==============================] - 43s 125ms/step - loss: 0.6516 - acc: 0.7833\n",
      "Epoch 6/50\n",
      "344/344 [==============================] - 44s 126ms/step - loss: 0.5585 - acc: 0.8144\n",
      "Epoch 7/50\n",
      "344/344 [==============================] - 44s 127ms/step - loss: 0.5038 - acc: 0.8357\n",
      "Epoch 8/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.4444 - acc: 0.8558\n",
      "Epoch 9/50\n",
      "344/344 [==============================] - 41s 119ms/step - loss: 0.4141 - acc: 0.8636\n",
      "Epoch 10/50\n",
      "344/344 [==============================] - 41s 118ms/step - loss: 0.3865 - acc: 0.8737\n",
      "Epoch 11/50\n",
      "344/344 [==============================] - 40s 117ms/step - loss: 0.3561 - acc: 0.8827\n",
      "Epoch 12/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.3517 - acc: 0.8876\n",
      "Epoch 13/50\n",
      "344/344 [==============================] - 41s 118ms/step - loss: 0.3195 - acc: 0.8973\n",
      "Epoch 14/50\n",
      "344/344 [==============================] - 40s 115ms/step - loss: 0.3009 - acc: 0.9025\n",
      "Epoch 15/50\n",
      "344/344 [==============================] - 40s 115ms/step - loss: 0.3011 - acc: 0.9025\n",
      "Epoch 16/50\n",
      "344/344 [==============================] - 41s 119ms/step - loss: 0.2837 - acc: 0.9120\n",
      "Epoch 17/50\n",
      "344/344 [==============================] - 41s 118ms/step - loss: 0.2744 - acc: 0.9109\n",
      "Epoch 18/50\n",
      "344/344 [==============================] - 39s 115ms/step - loss: 0.2863 - acc: 0.9113\n",
      "Epoch 19/50\n",
      "344/344 [==============================] - 39s 114ms/step - loss: 0.2679 - acc: 0.9153\n",
      "Epoch 20/50\n",
      "344/344 [==============================] - 39s 114ms/step - loss: 0.2501 - acc: 0.9199\n",
      "Epoch 21/50\n",
      "344/344 [==============================] - 39s 114ms/step - loss: 0.2403 - acc: 0.9248\n",
      "Epoch 22/50\n",
      "344/344 [==============================] - 39s 114ms/step - loss: 0.2300 - acc: 0.9275\n",
      "Epoch 23/50\n",
      "344/344 [==============================] - 39s 114ms/step - loss: 0.2300 - acc: 0.9266\n",
      "Epoch 24/50\n",
      "344/344 [==============================] - 40s 115ms/step - loss: 0.2243 - acc: 0.9260\n",
      "Epoch 25/50\n",
      "344/344 [==============================] - 41s 120ms/step - loss: 0.2209 - acc: 0.9301\n",
      "Epoch 26/50\n",
      "344/344 [==============================] - 40s 118ms/step - loss: 0.2120 - acc: 0.9358\n",
      "Epoch 27/50\n",
      "344/344 [==============================] - 39s 114ms/step - loss: 0.1994 - acc: 0.9359\n",
      "Epoch 28/50\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 0.2016 - acc: 0.9348\n",
      "Epoch 29/50\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 0.2073 - acc: 0.9351\n",
      "Epoch 30/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.2015 - acc: 0.9379\n",
      "Epoch 31/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1943 - acc: 0.9401\n",
      "Epoch 32/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.2002 - acc: 0.9374\n",
      "Epoch 33/50\n",
      "344/344 [==============================] - 42s 122ms/step - loss: 0.1846 - acc: 0.9434\n",
      "Epoch 34/50\n",
      "344/344 [==============================] - 41s 121ms/step - loss: 0.1897 - acc: 0.9430\n",
      "Epoch 35/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1799 - acc: 0.9443\n",
      "Epoch 36/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1822 - acc: 0.9451\n",
      "Epoch 37/50\n",
      "344/344 [==============================] - 42s 122ms/step - loss: 0.1719 - acc: 0.9488\n",
      "Epoch 38/50\n",
      "344/344 [==============================] - 42s 122ms/step - loss: 0.1888 - acc: 0.9454\n",
      "Epoch 39/50\n",
      "344/344 [==============================] - 42s 122ms/step - loss: 0.1609 - acc: 0.9518\n",
      "Epoch 40/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1730 - acc: 0.9477\n",
      "Epoch 41/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1587 - acc: 0.9511\n",
      "Epoch 42/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1722 - acc: 0.9515\n",
      "Epoch 43/50\n",
      "344/344 [==============================] - 41s 121ms/step - loss: 0.1577 - acc: 0.9519\n",
      "Epoch 44/50\n",
      "344/344 [==============================] - 41s 121ms/step - loss: 0.1552 - acc: 0.9543\n",
      "Epoch 45/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1611 - acc: 0.9528\n",
      "Epoch 46/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1502 - acc: 0.9558\n",
      "Epoch 47/50\n",
      "344/344 [==============================] - 41s 120ms/step - loss: 0.1446 - acc: 0.9573\n",
      "Epoch 48/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1540 - acc: 0.9570\n",
      "Epoch 49/50\n",
      "344/344 [==============================] - 42s 121ms/step - loss: 0.1547 - acc: 0.9567\n",
      "Epoch 50/50\n",
      "344/344 [==============================] - 42s 122ms/step - loss: 0.1396 - acc: 0.9608\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import h5py as h5py\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', \n",
    "                          patience = 5, \n",
    "                          verbose = 1,\n",
    "                          min_delta = 0.01, \n",
    "                          mode = 'min'), \n",
    "            ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                          factor = 0.1, \n",
    "                          patience = 3, \n",
    "                          verbose = 1, \n",
    "                          epsilon = 0.01,  \n",
    "                          mode = 'min'), \n",
    "            ModelCheckpoint(monitor = 'val_loss', \n",
    "                          filepath = 'weights/starter.hdf5', \n",
    "                          save_best_only = True, \n",
    "                          save_weights_only = True, \n",
    "                          mode = 'min')]\n",
    "# TQDMNotebookCallback()\n",
    "\n",
    "epochSize = 50\n",
    "steps_per_epoch = 344\n",
    "history = model.fit_generator(generator = train_generator(64), \n",
    "                             steps_per_epoch = steps_per_epoch, \n",
    "                             epochs = epochSize,\n",
    "                             verbose = 1)#, \n",
    "                             #callbacks = callbacks)#,\n",
    "                             #validation_data = valid_generator(64), \n",
    "                             #validation_steps = int(np.ceil(valid_df.shape[0]/64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#model.load_weights('./weights/starter.hdf5')\n",
    "now = datetime.datetime.now()\n",
    "saveFileName = str(epochSize)+'_'+str(steps_per_epoch)+'.h5' \n",
    "model.save('model/'+saveFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
